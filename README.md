# Predicting Higgs Boson with Machine Learning
The Higgs boson is a particle which gives mass to other elementary particles. In this project, we are using data provided by the ATLAS collaboration at CERN and perform machine learning methods to predict signal or background from an observed event.

## Table of contents
* [Setup](#Setup)
* [Running instructions](#Running instructions)
    * [a) run.py](#run.py)
    * [b) hyperparameters.py](#hyperparameters.py)
    * [c) training.py](#training.py)
    * [d) plots.py](#plots.py)
* [Example workflow](#Example workflow)
* [File structure](#File structure)


## Setup

```
numpy 1.19.2
matplotlib
```

Numpy is the one dependency to run all the code that is apart of the machine learning pipeline as stated in the project requirements. We only use plotting libraries like matplotlib to create plots for our report. 
```
timeit
default_timer
os
path
argparse
```
Python Standard Library: `timeit` is used to give feedback on running time of various processes. `os` and `argparse` are used to take in user input from the command line to make selections and run the various programs. 

## Running instructions
### a) run.py
**Overview:** `run.py` takes optimized weights from a .json file and uses them to predict labels for the test data set. 

**Dependency:** 
* `./Data/` directory contains the training and test files, which need to be named 'test.csv' and 'train.csv'. Do not change the name of the directory 'Data/'. 
* There must be a corresponding weights file. We provide for our submission 'weights_regularized_logistic.json'.

**Run** `run.py` using the command line:
```
$ python3 run.py
```
This will use optimized weights from weight_regularized_logistic.json to generate the same results as in 'submission.csv', which uses Regularized Logistic Regression. This .json file was generated by going to the pipeline listed below.

**Outcomes:** it will write the results to a file named 'submission.csv', saved at the project root directory.


### b) hyperparameters.py
**Overview:** `hyperparameters.py` loads a set of hyperparameters from 'init_hyperparams.json', grid searches accross all combinations of hyperparameters using K-Fold Cross Validation and returns the combination with the best accuracy.

**Dependency:** 
* In `./hyperparams_weights/init_hyperparams.json` adjust the hyperparameters for all the models or the model of interest. Do not edit the keys as the keys represent the hyperparameters. 
* In `./helper_files/data_io.py` is where a few data processing parameters can be adjusted. First in `load_csv_data()` (line 20) you can adjust the deletion of column by changing the line 
    ```
    input_data = np.delete(input_data, [_ for _ in range(12,30)], axis=1)
    ```
* Next, in `read_training_set()` you can adjust `sub_sample = False or True` (line 99) depending on if you want the full data or a subset.

**Run** `hyperparameters.py` using the command line:
```
$ python3 hyperparams.py -m <name_of_model>
```
Where `<name_of_model>` is a choice between: 'gd', 'sgd', 'ridge', 'least_squares', 'logistic' and 'regularized_logistic', corresponding to each machine learning method.

**Outcomes:** the best performing set of hyperparameters will be saved at `./hyperparams_weights`, with the file name `best_hyperparameters_{model}.json` according to the method. 


### c) training.py
**Overview:** `training.py` uses the best hyperparameters from `/hyperparams_weights` computed previously in 'hyperparameters.py' and trains the model using the entire training data set to find the optimized weights.

**Dependency:** 
* takes `./hyperparams_weights/best_hyperparams_{model}.json` as the hyperparameters with which the weights are optimized. 

**Run** `training.py` using the command line:
```
$ python3 training.py -m <name_of_model>
```
Where `<name_of_model>` is a choice between: 'gd', 'sgd', 'ridge', 'least_squares', 'logistic' and 'regularized_logistic', corresponding to each machine learning method.

**Outcomes:** the optimized weights and corresponding hyperparameters will be saved at `./hyperparams_weights`, with the file name `weights_{model}.json` according to the method. 


### d) plots.py
**Overview:** `plots.py` generates a boxplot or a learning curve plot using the best performing hyperparameters and K-Fold Cross Validation.

**Dependency:** 
* ./hyperparams_weights/best_hyperparams_{model}.json` 

**Run** `plots.py` using the command line:
```
$ python3 plots.py -p <name_of_plot>
```
where `<name_of_plot>` is a choice between 'box' (boxplot comparing accuracy of all models), 'gd', 'sgd', 'logistic', 'regularized_logistic' (for the learning curve of the corresponding model).

**Outcomes:** the generated learning curve is saved in `./img` as 'Learning_Curve_{model}_{hyperparameters}'. The generated boxplot is just shown.


## Example workflow
In this section we show an example of the workflow one would go through to iterate and improve machine learning predictions. 
Linear regression using Gradient Descent is used for this example.
1. Check that 'train.csv' and 'test.csv' are in `./Data`.
2. Adjust hyperparameters in `./hyperparams_weights/init_hyperparams.json`
3. Run hyperparameters.py
    ```
    $ python3 hyperparams.py -m gradient_descent
    ```
4. 'best_hyperparams_gradient_descent.json' will be saved at `./hyperparams_weights`
5. Run plot.py 
    ```
    $ python3 plot.py -p gd
    ```
6. The generated plot will be saved in `./img` as 'Learning_Curve_{model}_{hyperparameters}'
7. Run training.py
    ```
    $ python3 training.py -m gradient_descent
    ```
8. 'weights_least_squares.json' will be saved at `./hyperparams_weights`
9. Run run.py stating the argument for gradient descent (the default uses regularized logistic regression).
    ```
    $ python3 run.py -m gradient_descent
    ```
10. Results will be saved to a file named 'submission.csv' at the project root directory.



## File Structure
Here is the file structure of the project: 
```bash
Project
|
|--README.md
|-- submission.csv  
|
|-- Data
|   |-- test.csv
|   |-- train.csv
|   |-- sample-submission.csv
|
|-- hyperparams_weights : contains the files with the initial set of hyperparameters, the best performing hyperparameters for each model and the calculated weights for the given data
|    |-- best_hyperparams_gd.json 
|    |-- best_hyperparams_least_squares.json
|    |-- best_hyperparams_logistic.json
|    |-- best_hyperparams_regularized_logistic.json
|    |-- best_hyperparams_ridge.json
|    |-- best_hyperparams_sgd.json
|    |-- init_hyperparams.json
|    |-- weights_gd.json  
|    |-- weights_least_squares.json
|    |-- weights_logistic.json
|    |-- weights_regularized_logistic.json
|    |-- weights_ridge.json
|    |-- weights_sgd.json
|
|-- helper_files : contains helper function files
|    |-- costs.py : functions to compute the costs
|    |-- data_io.py : functions to input and output data, e.g., load and save files
|    |-- data_pre_process.py : functions used to pre-process data (standardization, normalization and imputation)
|    |-- helpers.py : helper functions 
|    |-- kfold_cv.py : used to generate the combinations of hyperparameters and perform k-fold cross validation for each model
|   |-- implementations_private.py : contains the machine learning methods used to train the data (gradient descent, stochastic gradient descent, least squares, ridge regression, logistic regression and regularized logistic regression)
|
|-- img : contains the images of the plots
|
|-- run.py : used to make the predictions and submit them to the platform competition
|-- implementations.py : contains the machine learning methods necessary for the submission
|-- hyperparams.py : used to find the best performing set of hyperparameters for a given model using K-Fold Cross Validation and save them
|-- training.py : used to train the data, finds and saves the weights corresponding to the best performing set of hyperparameters
|-- plots.py : generates plots showing the accuracy of each model and learning curves

```



